{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparation des données\n",
    "\n",
    "- trouver/supprimer les données dupliquées : df.duplicated() / df.drop_duplicates()\n",
    "- renommer des colonnes : df.rename(columns={...))\n",
    "- trouver les NaN : df.isnull() / df.notnull() / df.dropna()\n",
    "- travail sur les chaines : series.str.extract(), series.str.contains(), series.get_dummies()\n",
    "- mapping : series.map()\n",
    "- changer le type d'une série (cast) : df.astype(type) / pd.to_numeric() / pd.to_datetime()\n",
    "- remplacer n'importe quelle valeur : df.replace({...})\n",
    "- remplacer les NaN : df.fillna(), series.combine_first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chargement et analyse des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('people.csv')\n",
    "df0 = df.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplications\n",
    "\n",
    "- `duplicated()` : `True` ou `False` selon si une ligne est dupliquée\n",
    "- `drop_duplicates()` : suppression des lignes dupliquées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lignes dupliquées\n",
    "df.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toutes les lignes dupliquées\n",
    "df.loc[df.duplicated(keep=False)].sort_values('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des lignes dupliquées\n",
    "df = df.drop_duplicates()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc\n",
    "#df.drop_duplicates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renommage de la colonne 'email address'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renommer les colonnes\n",
    "df = df.rename(columns={'email address': 'email'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse des données manquantes\n",
    "\n",
    "`numpy.nan` est utilisé dans **pandas** pour représenter des valeurs manquantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not A Number\n",
    "np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type\n",
    "type(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# élément super absorbant\n",
    "np.nan + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# élément super absorbant\n",
    "np.sqrt(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# élément super absorbant\n",
    "np.nan == np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nan > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# au passage infinis numpy\n",
    "np.NINF, np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.inf > 1e100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.inf + 1e100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.inf == np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.inf + np.NINF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.inf + np.inf > np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tests sur les données manquantes\n",
    "\n",
    "- `isnull()` ou `isna()`\n",
    "- `notnull()` ou `notna()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ne fonctionne pas\n",
    "df.loc[df['first_name']==np.nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chercher les first_name Nan\n",
    "df.loc[df['first_name'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sur tout le dataframe\n",
    "df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chercher tous les lignes avec au moins un NaN\n",
    "df.loc[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chercher tous les lignes avec que des NaN\n",
    "df.loc[df.isnull().all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression de toutes les lignes avec un NaN\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression de toutes les lignes avec un NaN\n",
    "df.dropna().isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supprimer uniquement les lignes dont le first_name NaN \n",
    "df = df.dropna(subset=['first_name'])\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajout d'une colonne 'full_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'full_name'  = 'first_name last_name'\n",
    "df['full_name'] = df['first_name'] + ' ' + df['last_name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse de la colonne 'address'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse de address\n",
    "df['address'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ajout des colonnes 'city' et 'country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul de city et country à partir de address\n",
    "df[['city', 'country']] = df['address'].str.extract('(.*), (.*)')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nunique : modalités par colonne\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping du genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse du gender\n",
    "df['gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse du gender\n",
    "df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# traitement du gender\n",
    "mapping = {'Female': 'F', 'Male': 'M', 'F': 'F', 'M': 'M'}\n",
    "df['gender'] = df['gender'].map(mapping)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionnaire incomplet\n",
    "mapping0 = {'Female': 'F', 'Male': 'M'}\n",
    "s = df0['gender'].map(mapping0)\n",
    "df0['gender'].count(), s.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionnaire incomplet\n",
    "s = df0['gender'].apply(lambda x: mapping0.get(x, x))\n",
    "s.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitement du gender, map() avec une Series\n",
    "mapping = pd.Series({'Female': 'F', 'Male': 'M', 'F': 'F', 'M': 'M'})\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitement du gender avec une Series\n",
    "df['gender'] = df['gender'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# au final\n",
    "df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse du genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse gender NaN\n",
    "len(df0.loc[df0['gender'].isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse prénom avec gender NaN\n",
    "df0.loc[df0['gender'].isnull(), 'first_name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse gender\n",
    "df0.loc[df0['gender'].isnull(), 'first_name'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compléter le genre :\n",
    "\n",
    "1. Autocomplétion avec le fichier people.csv (mais très peu de cas)\n",
    "2. Gender API : https://gender-api.com/fr (simple mais API payante si gros volumes + de 500/mois)\n",
    "3. US SSA baby names : https://www.ssa.gov/oact/babynames/limits.html (\"gratuit\", stats à produire, éventuellement affiner par année de naissance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Fichier people.csv\n",
    "for first_name in df0.loc[df0['gender'].isnull(), 'first_name'].unique():\n",
    "    print(first_name, df0.loc[df0['first_name']==first_name, 'gender'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse de l'âge\n",
    "\n",
    "`pandas.Series.astype()` : types\n",
    "\n",
    "`pandas.to_numeric()` : data avec gestion des erreurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse de l'âge\n",
    "df.loc[df['age'].str.contains('[^0-9]'), 'age'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_numeric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traitement de l'âge\n",
    "df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traitement des dates\n",
    "\n",
    "\n",
    "`pandas.to_datetime()` : data, gestion des formats et des erreurs\n",
    "\n",
    "`pandas.Series.combine_first()` : équivalent à `fillna()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_time = ['registration', 'last_seen']\n",
    "df[cols_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[cols_time].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion des dates\n",
    "df['registration'] = pd.to_datetime(df['registration'])\n",
    "df['last_seen'] = pd.to_datetime(df['last_seen'], unit='s')\n",
    "# si last_seen est NaN, prendre registration\n",
    "df['last_seen'] = df['last_seen'].fillna(df['registration'])\n",
    "# idem\n",
    "df['last_seen'] = df['last_seen'].combine_first(df['registration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse de currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# échantillon\n",
    "np.random.seed(0)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traitement de 'currency'\n",
    "\n",
    "Produire une nouvelle colonne numérique 'money_eur'.\n",
    "\n",
    "Pour la conversion USD/EUR, on utilise l'API https://api.exchangeratesapi.io/latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API\n",
    "import json\n",
    "\n",
    "import requests\n",
    "\n",
    "response = requests.get('https://open.er-api.com/v6/latest/EUR')\n",
    "rates = json.loads(response.content)\n",
    "rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['money'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['currency'] = df['money'].str[0].map({'€': 'EUR', '$': 'USD'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rates['rates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction de la currency\n",
    "df['currency'] = df['money'].str[0].map({'€': 'EUR', '$': 'USD', '£': 'GBP'})\n",
    "df['money_eur'] = df['money'].str[1:].str.replace(',', '.')  # extraction des derniers chars + , => .\n",
    "df['money_eur'] = pd.to_numeric(df['money_eur'])  # conversion en nombre\n",
    "\n",
    "# conversion des monnaies en euros\n",
    "df['money_eur'] = df['money_eur'] / df['currency'].map(rates['rates'])\n",
    "#np.random.seed(0)\n",
    "#df.sample(10)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['money_eur'] = df['money_eur'].apply(lambda x: round(x, 2))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse des emails\n",
    "\n",
    "On va utiliser des regex pour nettoyer les emails mais mieux vaut utiliser une librairie spécialisée. Par exemple, https://github.com/syrusakbary/validate_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# email NaN\n",
    "df['email'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des emails absents\n",
    "df = df.dropna(subset=['email']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emails avec chars non admis\n",
    "df.loc[df['email'].str.contains('[^A-Za-z0-9_\\-%+.@]'), 'email'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des blancs\n",
    "df['email'] = df['email'].str.strip()\n",
    "df.loc[df['email'].str.contains('[^A-Za-z0-9_\\-%+.@]'), 'email']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex pour vérifier les domaines\n",
    "df.loc[~df['email'].str.contains('.+@[A-Za-z0-9_\\-.]+\\.[A-Za-z]{2,}')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emails avec noms de domaine invalides\n",
    "df = df.loc[df['email'].str.contains('.+@[A-Za-z0-9_\\-.]+\\.[A-Za-z]{2,}')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emails avec aliases (char +)\n",
    "df.loc[df['email'].str.contains('\\+'), 'email'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction des aliases (char +)\n",
    "tab_email = df['email'].str.extract('([^+]+)(\\+.*)?(@.+)').sort_values(0)\n",
    "tab_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '^...' regex au début"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [abc]\n",
    "# [a-zA-Z]\n",
    "# [^abc] : pas a, b ou c\n",
    "# [^0-9] : pas digit\n",
    "# '<[^>]+>' : tag HTML\n",
    "# '<.+>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# caractères parenthèses\n",
    "# \\([0-9]+\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parenthèses non capturante\n",
    "# (?:regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suprresion des aliases (char +)\n",
    "df['email'] = tab_email[0] + tab_email[2]\n",
    "df.sort_values('email')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppression des emails en double, on conserve la première ligne\n",
    "df = df.drop_duplicates(subset=['email'])\n",
    "df.sort_values('email')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse de la colonne 'preference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse de preference\n",
    "df['preference'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyse de preference\n",
    "df['preference'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modalités de preference\n",
    "s = set()\n",
    "df['preference'].apply(lambda x: s.update(x.split('/')))\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajour d'un booléen par preference\n",
    "for x in sorted(s):\n",
    "    df[x] = df['preference'].str.contains(x)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autre façon avec get_dummies\n",
    "df['preference'].str.get_dummies(sep='/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assignation des préférences\n",
    "tab_preference = df['preference'].str.get_dummies(sep='/')\n",
    "df[tab_preference.columns] = tab_preference.astype(bool)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def clean_people(df):\n",
    "    \n",
    "    # suppression des lignes dupliquées\n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # renommer les colonnes\n",
    "    df = df.rename(columns={'email address': 'email'})\n",
    "    \n",
    "    # supprimer uniquement les lignes dont le first_name vaut NaN \n",
    "    df = df.dropna(subset=['first_name'])\n",
    "    \n",
    "    # ajout d'une colonne 'full_name'\n",
    "    df['full_name'] = df['first_name'] + ' ' + df['last_name']\n",
    "\n",
    "    # calcul de city et country à partir de address\n",
    "    df[['city', 'country']] = df['address'].str.extract('(.*), (.*)')\n",
    "\n",
    "    # traitement du gender\n",
    "    mapping = {'Female': 'F', 'Male': 'M'}\n",
    "    df['gender'] = df['gender'].map(mapping)\n",
    " \n",
    "    # traitement de l'âge\n",
    "    df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "\n",
    "    # conversion des dates\n",
    "    df['registration'] = pd.to_datetime(df.registration)\n",
    "    df['last_seen'] = pd.to_datetime(df.last_seen, unit='s')\n",
    "    # si last_seen est NaN, prendre registration\n",
    "    df['last_seen'] = df['last_seen'].fillna(df['registration'])\n",
    "\n",
    "    # récupération des taux de change\n",
    "    response = requests.get('https://open.er-api.com/v6/latest/EUR')\n",
    "    rates = json.loads(response.content)\n",
    "\n",
    "    # extraction de la currency\n",
    "    df['currency'] = df['money'].str[0].map({'€': 'EUR', '$': 'USD'})\n",
    "    df['money_eur'] = df['money'].str[1:].str.replace(',', '.')  # extraction des derniers chars + , => .\n",
    "    df['money_eur'] = pd.to_numeric(df['money_eur'])  # conversion en nombre\n",
    "\n",
    "    # conversion des monnaies en euros\n",
    "    rates['rates']['EUR'] = 1.0  # ajour de EUR pour pouvoir utiliser map()\n",
    "    df['money_eur'] = df['money_eur'] * df['currency'].map(rates['rates'])\n",
    "\n",
    "    # suppression des emails absents\n",
    "    df = df.dropna(subset=['email'])\n",
    "\n",
    "    # suppression des blancs\n",
    "    df['email'] = df['email'].str.strip()\n",
    "\n",
    "    # emails avec noms de domaine valides\n",
    "    df = df.loc[df['email'].str.contains('.+@[A-Za-z0-9_\\-.]+\\.[A-Za-z]{2,}')]\n",
    "    \n",
    "    # extraction des aliases (char +)\n",
    "    tab_email = df['email'].str.extract('([^+]+)(\\+.*)?(@.+)').sort_values(0)\n",
    "    # suppression des aliases (char +)\n",
    "    df['email'] = tab_email[0] + tab_email[2]\n",
    "\n",
    "    # suppression des emails en double, on conserve la première ligne\n",
    "    df = df.drop_duplicates(subset=['email'])\n",
    "\n",
    "    # assignation des préférences\n",
    "    tab_preference = df['preference'].str.get_dummies(sep='/')\n",
    "    df[tab_preference.columns] = tab_preference.astype(bool)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('people.csv')\n",
    "print(df.shape)\n",
    "\n",
    "df = clean_people(df)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice**\n",
    "\n",
    "1. Téléchargez le fichier Excel \"FranceTHD_Open_Data_Observatoire_2017_T2.xlsx\" sur le niveau des débits sur les réseaux d'accès à Internet : ADSL, câble, Fibre FttH (T2 2015 - T2 2017) de la page : https://www.data.gouv.fr/fr/datasets/niveau-des-debits-sur-les-reseaux-dacces-a-internet-adsl-cable-fibre-ftth-t2-2015-t2-2017/\n",
    "\n",
    "2. Chargez avec pd.read_excel() dans un DataFrame le dernier onglet \"2017 T2\" en mesurant le temps avec %%time en première instruction de cellule.\n",
    "\n",
    "3. Modifiez le nom des 4 premières colonnes en : 'code INSEE', 'commune', 'département', 'nb locaux' par exemple.\n",
    "\n",
    "4. Sauvegardez le DataFrame avec pd.to_pickle().\n",
    "\n",
    "5. Rechargez le DataFrame à partir du fichier pickle en mesurant le temps avec %%time en première instruction de cellule et comparez.\n",
    "\n",
    "6. Effectuez une opération de sélection sur les communes : par exemple, les communes qui commencent par \"SAINT\".\n",
    "\n",
    "7. Diagnostiquez le message d'erreur.\n",
    "\n",
    "8. Corrigez le DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Unnamed: 0               Unnamed: 1 Unnamed: 2  Unnamed: 3  éligibles  \\\n",
      "0      01001  L'Abergement-Clémenciat         01         361      1.000   \n",
      "1      01002    L'Abergement-de-Varey         01         168      0.288   \n",
      "2      01004        Ambérieu-en-Bugey         01        7501      0.998   \n",
      "3      01005      Ambérieux-en-Dombes         01         725      1.000   \n",
      "4      01006                  Ambléon         01          73      1.000   \n",
      "\n",
      "   3M et +  8M et +  30M et +  100M et +  éligibles.1  ...  éligibles.2  \\\n",
      "0    0.456    0.052     0.000      0.000        1.000  ...          0.0   \n",
      "1    0.076    0.024     0.024      0.024        0.276  ...          0.0   \n",
      "2    0.974    0.898     0.430      0.198        0.998  ...          0.0   \n",
      "3    0.993    0.945     0.667      0.036        1.000  ...          0.0   \n",
      "4    1.000    1.000     1.000      1.000        1.000  ...          0.0   \n",
      "\n",
      "   3M et +.2  8M et +.2  30M et +.2  100M et +.2  éligibles.3  3M et +.3  \\\n",
      "0        0.0        0.0         0.0          0.0        0.000      0.000   \n",
      "1        0.0        0.0         0.0          0.0        0.024      0.024   \n",
      "2        0.0        0.0         0.0          0.0        0.198      0.198   \n",
      "3        0.0        0.0         0.0          0.0        0.036      0.036   \n",
      "4        0.0        0.0         0.0          0.0        1.000      1.000   \n",
      "\n",
      "   8M et +.3  30M et +.3  100M et +.3  \n",
      "0      0.000       0.000        0.000  \n",
      "1      0.024       0.024        0.024  \n",
      "2      0.198       0.198        0.198  \n",
      "3      0.036       0.036        0.036  \n",
      "4      1.000       1.000        1.000  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = pd.read_excel('FranceTHD_Open_Data_Observatoire_2017_T2.xlsx',\n",
    "               sheet_name=-1,\n",
    "               header=1)\n",
    "df\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Etudier les multiples options de read_csv\n",
    "\n",
    "En particulier:\n",
    "\n",
    "<pre>\n",
    "pd.read_csv(\n",
    "    <strong>filepath_or_buffer: Union[str, pathlib.Path, IO[~AnyStr]],</strong>\n",
    "    <strong>sep=',',</strong>\n",
    "    delimiter=None,\n",
    "    <strong>header='infer',</strong>\n",
    "    <strong>names=None,</strong>\n",
    "    <strong>index_col=None,</strong>\n",
    "    <strong>usecols=None,</strong>\n",
    "    squeeze=False,\n",
    "    prefix=None,\n",
    "    mangle_dupe_cols=True,\n",
    "    <strong>dtype=None,</strong>\n",
    "    <strong>engine=None,</strong>\n",
    "    <strong>converters=None,</strong>\n",
    "    true_values=None,\n",
    "    false_values=None,\n",
    "    skipinitialspace=False,\n",
    "    <strong>skiprows=None,</strong>\n",
    "    <strong>skipfooter=0,</strong>\n",
    "    <strong>nrows=None,</strong>\n",
    "    <strong>na_values=None,</strong>\n",
    "    <strong>keep_default_na=True,</strong>\n",
    "    na_filter=True,\n",
    "    verbose=False,\n",
    "    skip_blank_lines=True,\n",
    "    <strong>parse_dates=False,</strong>\n",
    "    infer_datetime_format=False,\n",
    "    keep_date_col=False,\n",
    "    date_parser=None,\n",
    "    dayfirst=False,\n",
    "    cache_dates=True,\n",
    "    iterator=False,\n",
    "    <strong>chunksize=None,</strong>\n",
    "    compression='infer',\n",
    "    <strong>thousands=None,</strong>\n",
    "    <strong>decimal: str = '.',</strong>\n",
    "    lineterminator=None,\n",
    "    quotechar='\"',\n",
    "    quoting=0,\n",
    "    doublequote=True,\n",
    "    escapechar=None,\n",
    "    comment=None,\n",
    "    encoding=None,\n",
    "    dialect=None,\n",
    "    error_bad_lines=True,\n",
    "    warn_bad_lines=True,\n",
    "    delim_whitespace=False,\n",
    "    low_memory=True,\n",
    "    memory_map=False,\n",
    "    float_precision=None,\n",
    ")\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.read_csv?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse automatique avec pandas_profiling\n",
    "\n",
    "https://github.com/pandas-profiling/pandas-profiling\n",
    "\n",
    "**ATTENTION, il vaut mieux installer `pandas_profiling` dans un nouvel environnement**\n",
    "\n",
    "<pre>\n",
    "conda create --name profiling\n",
    "\n",
    "activate profiling OU conda activate profiling\n",
    "\n",
    "conda install -c conda-forge pandas-profiling\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiling raw people\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "df = pd.read_csv('people.csv')\n",
    "\n",
    "profile = ProfileReport(df, title='Pandas Profiling Report', explorative=True)\n",
    "\n",
    "profile.to_file(\"people.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profiling clean people\n",
    "profile = ProfileReport(clean_people(df), title='Pandas Profiling Report', explorative=True)\n",
    "\n",
    "profile.to_file(\"clean_people.html\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "434071a3f25b08b91dbd65f7dcb70badbb29f41fd9ba8d98c73d955f26f14959"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
